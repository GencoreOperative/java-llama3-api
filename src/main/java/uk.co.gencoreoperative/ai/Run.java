package uk.co.gencoreoperative.ai;

import java.time.Duration;
import java.util.stream.Stream;

import javax.annotation.Nonnull;

import uk.co.gencoreoperative.Config;

/**
 * Represents the ability to run an AI request and get the response from that request.
 */
public interface Run {
    /**
     * Perform a single request and wait for the complete response from the LLM before returning.
     *
     * @param options The {@link Config} object containing the system prompt, user prompt, and other settings.
     * @return The output of the model if any.
     */
    @Nonnull String run(@Nonnull Config options) throws RuntimeException;

    /**
     * Invoke the LLM to perform a request and return immediately to the caller.
     * <p>
     * This method will run asynchronously and will stream the characters generated by the LLM
     * to the {@link Stream}.
     *
     * @param options The {@link Config} object containing the system prompt, user prompt, and other settings.
     * @return A {@link Stream} of characters generated by the model.
     * @throws RuntimeException if the model invocation fails or an error occurs during processing.
     */
    @Nonnull Stream<String> runAsStream(@Nonnull Config options) throws RuntimeException;

    /**
     * Invoke the LLM and wait for the complete response allowing us to parse out additional statistics
     * from the run.
     * <p>
     * This method provides details about the LLM invocation including {@link ContextWindow} and {@link Duration}.
     *
     * @param options The {@link Config} controlling the LLM invocation.
     * @return A {@link Response} object containing the model's output, context window, and metadata.
     * @throws RuntimeException if the model invocation fails or an error occurs during processing.
     */
    @Nonnull Response runWithResponse(@Nonnull Config options) throws RuntimeException;
}
